# Q1: Tokenization and Masked Prediction Assignment

## Setup
Install the required libraries:
```bash
pip install tokenizers transformers sentencepiece
```

## Files
- `tokenise.py`: Tokenizes the sentence using BPE (roberta-base), WordPiece (bert-base-uncased), and SentencePiece (albert-base-v2). Also performs masking and prediction using bert-base-uncased.
- `predictions.json`: Top-3 predictions for each mask (generated by the script).
- `compare.md`: Comparison of tokenization results and brief explanation.

## Usage
Run the script:
```bash
python q1/tokenise.py
```

## Notes
- The script uses `bert-base-uncased` for fill-mask prediction, which works on most machines.
- For BPE and SentencePiece tokenization, `roberta-base` and `albert-base-v2` tokenizers are used for reporting only.
- The script saves tokenization results and predictions in JSON files for easy reporting.
- If you want to try a larger model, see the comments in the script, but be aware of hardware requirements. 